{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¿ Project: Hybrid Anime Recommender System\n",
        "### *Bridging Collaborative Filtering & Semantic Search*\n",
        "\n",
        "### **The Challenge**\n",
        "Building a recommendation system is often a trade-off.\n",
        "* **Collaborative Filtering** is great at finding patterns (*\"People who liked X also liked Y\"*), but it fails with new items (Cold Start) and doesn't understand content.\n",
        "* **Content-Based Filtering** understands metadata, but often fails to capture user nuance.\n",
        "\n",
        "### **The Solution: A Hybrid Architecture**\n",
        "In this notebook, I am building a dual-engine system that combines the best of both worlds:\n",
        "1.  **The \"User Expert\" (Neural Network):** A Deep Learning model (Neural Collaborative Filtering) that learns to predict user ratings based on interaction history.\n",
        "2.  **The \"Content Expert\" (BERT):** A Transformer-based model that reads anime plot summaries to understand semantic similarity, powering a \"Smart Search\" feature."
      ],
      "metadata": {
        "id": "Paoh_n0VT1YW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV6LJJtwTQU4"
      },
      "outputs": [],
      "source": [
        "# First, I need to set up the environment.\n",
        "# I'll be using TensorFlow for the Neural Network, Sentence-Transformers for BERT,\n",
        "# and FAISS for high-performance vector search.\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import faiss\n",
        "import kagglehub\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
        "\n",
        "print(\"âœ… Environment is set up and ready to go.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phase 1: Data Acquisition & Preprocessing**\n",
        "\n",
        "High-quality data is the foundation of any good model. I am using the **Anime Recommendation Database 2020**, which contains millions of user ratings and detailed metadata.\n",
        "\n",
        "**My Strategy:**\n",
        "* **Data Ingestion:** Download fresh data directly via API.\n",
        "* **Normalization:** The dataset uses non-standard column names (like `MAL_ID`), so I'll standardize everything to `anime_id`.\n",
        "* **Noise Reduction:** To ensure the model learns meaningful patterns, I will filter out \"sparse\" dataâ€”users who have rated fewer than 50 anime, and anime that have fewer than 50 ratings. This removes noise and stabilizes the training process."
      ],
      "metadata": {
        "id": "JAxXue02UIU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I'll grab the dataset directly from Kaggle to ensure reproducibility.\n",
        "print(\"â¬‡ï¸ Downloading Dataset...\")\n",
        "path = kagglehub.dataset_download(\"hernan4444/anime-recommendation-database-2020\")\n",
        "\n",
        "# Loading the specific files I need: Metadata and User Ratings.\n",
        "anime_df = pd.read_csv(os.path.join(path, 'anime_with_synopsis.csv'))\n",
        "rating_df = pd.read_csv(os.path.join(path, 'rating_complete.csv'))\n",
        "\n",
        "# The dataset uses 'MAL_ID', but I prefer 'anime_id' for clarity in my codebase.\n",
        "anime_df = anime_df.rename(columns={'MAL_ID': 'anime_id'})\n",
        "print(\"âœ… Column names standardized.\")\n",
        "\n",
        "# --- Filtering Strategy ---\n",
        "# I want to train on 'active' users and 'known' items to avoid the cold-start noise.\n",
        "min_ratings = 50\n",
        "\n",
        "# Counting interactions\n",
        "user_counts = rating_df['user_id'].value_counts()\n",
        "anime_counts = rating_df['anime_id'].value_counts()\n",
        "\n",
        "# Identifying valid candidates\n",
        "valid_users = user_counts[user_counts >= min_ratings].index\n",
        "valid_animes = anime_counts[anime_counts >= min_ratings].index\n",
        "\n",
        "# Filtering the dataframes\n",
        "df_filtered = rating_df[\n",
        "    (rating_df['user_id'].isin(valid_users)) &\n",
        "    (rating_df['anime_id'].isin(valid_animes))\n",
        "].copy()\n",
        "\n",
        "# Syncing the metadata to match the filtered ratings\n",
        "anime_df = anime_df[anime_df['anime_id'].isin(valid_animes)].copy()\n",
        "anime_df = anime_df.reset_index(drop=True)\n",
        "\n",
        "print(f\"âœ… Data Preprocessing Complete.\")\n",
        "print(f\"   Final Dataset: {len(anime_df)} Unique Animes, {len(df_filtered)} User Ratings.\")"
      ],
      "metadata": {
        "id": "1WoiHxFsUJhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phase 2: The \"Collaborative Brain\" (Neural Network)**\n",
        "\n",
        "Now I will build the **Neural Collaborative Filtering (NCF)** model.\n",
        "\n",
        "Instead of traditional Matrix Factorization, I am using a Deep Neural Network. This allows the model to learn non-linear relationships between users and items.\n",
        "\n",
        "**Architecture:**\n",
        "1.  **Input:** User ID and Anime ID.\n",
        "2.  **Embeddings:** These IDs are passed through Embedding Layers, which convert them into dense vectors (latent features).\n",
        "3.  **Concatenation:** I combine the User Vector and Item Vector.\n",
        "4.  **Dense Layers:** The combined vector passes through hidden layers to extract complex patterns.\n",
        "5.  **Output:** A single neuron with a `Sigmoid` activation, predicting a probability (0 to 1) of how much the user will like the item."
      ],
      "metadata": {
        "id": "SfHSV6oHURTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ§  Initializing Neural Collaborative Filtering (NCF) training...\")\n",
        "\n",
        "# 1. Encoding IDs\n",
        "# Neural networks require contiguous integers (0, 1, 2...), not random IDs.\n",
        "# So, I'll use LabelEncoder to map the real IDs to a sequence.\n",
        "user_enc = LabelEncoder()\n",
        "item_enc = LabelEncoder()\n",
        "\n",
        "df_filtered['user'] = user_enc.fit_transform(df_filtered['user_id'])\n",
        "df_filtered['item'] = item_enc.fit_transform(df_filtered['anime_id'])\n",
        "\n",
        "num_users = df_filtered['user'].nunique()\n",
        "num_items = df_filtered['item'].nunique()\n",
        "\n",
        "# 2. Defining the Model Architecture\n",
        "embedding_size = 32\n",
        "\n",
        "# Input Layers\n",
        "user_input = Input(shape=(1,), name='user_input')\n",
        "item_input = Input(shape=(1,), name='item_input')\n",
        "\n",
        "# Embedding Layers (The Latent Space)\n",
        "user_embedding = Embedding(num_users, embedding_size, name='user_embedding')(user_input)\n",
        "item_embedding = Embedding(num_items, embedding_size, name='item_embedding')(item_input)\n",
        "\n",
        "# Flattening vectors to prepare for concatenation\n",
        "user_vec = Flatten()(user_embedding)\n",
        "item_vec = Flatten()(item_embedding)\n",
        "\n",
        "# The Deep Learning part: Concatenate -> Dense Layers\n",
        "concat = Concatenate()([user_vec, item_vec])\n",
        "fc1 = Dense(128, activation='relu')(concat)\n",
        "fc2 = Dense(64, activation='relu')(fc1)\n",
        "output = Dense(1, activation='sigmoid')(fc2) # Output layer (Probability 0-1)\n",
        "\n",
        "# Compiling the model\n",
        "model = Model([user_input, item_input], output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 3. Training\n",
        "# I'm normalizing ratings to a 0-1 range to match the Sigmoid output.\n",
        "y_train = (df_filtered['rating'] - 1) / 9.0\n",
        "\n",
        "history = model.fit(\n",
        "    [df_filtered['user'], df_filtered['item']],\n",
        "    y_train,\n",
        "    batch_size=2048, # Large batch size for faster GPU processing\n",
        "    epochs=5,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "# 4. Exporting the Model\n",
        "# I'm saving this in the .h5 format to ensure compatibility with my backend.\n",
        "model.save('anime_neumf_model.h5')\n",
        "print(\"âœ… Collaborative Model successfully trained and exported.\")"
      ],
      "metadata": {
        "id": "dTP70mNOUTh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phase 3: The \"Semantic Brain\" (BERT & FAISS)**\n",
        "\n",
        "The Neural Network handles *users*, but it doesn't understand what an anime actually *is*. For that, I need **Natural Language Processing (NLP)**.\n",
        "\n",
        "**The Approach:**\n",
        "1.  **Soup Creation:** I will combine the Name, Genres, and Synopsis into a single text string (a \"soup\") for every anime.\n",
        "2.  **BERT Encoding:** I'll use `all-MiniLM-L6-v2`, a lightweight but powerful Transformer, to convert this text into a 384-dimensional vector.\n",
        "3.  **Indexing:** I'll store these vectors in a **FAISS Index**. This allows my application to perform sub-millisecond similarity searches (e.g., finding anime with similar plots)."
      ],
      "metadata": {
        "id": "sptWrKI6UYV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸš€ Launching BERT for Semantic Analysis...\")\n",
        "\n",
        "# 1. Loading the Pre-trained Transformer\n",
        "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Handling inconsistent column naming in the source dataset\n",
        "if 'sypnopsis' in anime_df.columns:\n",
        "    anime_df = anime_df.rename(columns={'sypnopsis': 'Synopsis'})\n",
        "elif 'synopsis' in anime_df.columns:\n",
        "    anime_df = anime_df.rename(columns={'synopsis': 'Synopsis'})\n",
        "\n",
        "# 2. Creating the 'Soup'\n",
        "# This combines all relevant text metadata into one string for BERT to read.\n",
        "anime_df['Genres'] = anime_df['Genres'].fillna('')\n",
        "anime_df['Synopsis'] = anime_df['Synopsis'].fillna('')\n",
        "anime_df['soup'] = anime_df['Name'] + \". \" + anime_df['Genres'] + \". \" + anime_df['Synopsis']\n",
        "\n",
        "# 3. Vectorization\n",
        "# This is the heavy lifting: converting text to numbers.\n",
        "print(\"â³ Generating Vector Embeddings...\")\n",
        "embeddings = bert.encode(\n",
        "    anime_df['soup'].tolist(),\n",
        "    batch_size=64,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "# 4. Building the FAISS Index\n",
        "# Normalizing L2 allows us to use Cosine Similarity logic with efficient Inner Product search.\n",
        "faiss.normalize_L2(embeddings)\n",
        "d = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(embeddings)\n",
        "\n",
        "# Exporting the Index\n",
        "faiss.write_index(index, \"anime_vector_db.index\")\n",
        "\n",
        "# 5. Packaging Metadata for Production\n",
        "# I need to save the Encoders and a lightweight dataframe so the API\n",
        "# can interpret the model's output (Mapping ID -> Name).\n",
        "indices = pd.Series(anime_df.index, index=anime_df['Name']).drop_duplicates().to_dict()\n",
        "\n",
        "metadata = {\n",
        "    'anime_df': anime_df[['anime_id', 'Name', 'Genres']],\n",
        "    'indices': indices,\n",
        "    'user_enc': user_enc,\n",
        "    'item_enc': item_enc\n",
        "}\n",
        "\n",
        "with open('production_metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(\"âœ… System Build Complete.\")\n",
        "print(\"   - Semantic Index saved: anime_vector_db.index\")\n",
        "print(\"   - Metadata Package saved: production_metadata.pkl\")"
      ],
      "metadata": {
        "id": "xPMeUFNFUau0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "The training pipeline is complete. We have successfully generated the three core artifacts required for the backend API:\n",
        "\n",
        "1.  `anime_neumf_model.h5`: The **Neural Network** that predicts user preference.\n",
        "2.  `anime_vector_db.index`: The **FAISS Index** that powers the semantic search.\n",
        "3.  `production_metadata.pkl`: The **Reference Data** (encoders and mappings).\n",
        "\n",
        "These files are now ready to be moved to the `/backend` directory for deployment."
      ],
      "metadata": {
        "id": "hq6bnuzPUg05"
      }
    }
  ]
}